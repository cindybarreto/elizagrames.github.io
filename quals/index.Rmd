---
title: "Qualifying Exam Notes"
author: "Eliza Grames"
bibliography: ../../master-bib.bib
output:
  html_notebook:
    collapsed: no
    smooth_scroll: no
    theme: journal
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---

# Checklist {.tabset .tabset-fade .tabset-pills}
## >>

## Checklist 

### Historical perspectives

&#9744; Foundations of Ecology

&#9744; On the Origin of Species

&#9745; Modeling Nature

&#9744; Classic papers

&#9744; Summaries of notable ecologists

### Current Topics

&#9744; Catch up on all journal articles in reading archive

### Ornithology

&#9744; Gill's Ornithology

&#9744; Birds and Habitat

&#9744; Summaries of notable ornithologists

### Conservation Biology

&#9744; Carrol and Fox Conservation Biology

&#9744; Primack cons bio

&#9744; Restoring North America's Birds

&#9744; Saving the World's Deciduous Forests

&#9744; Analysis and Management of Animal Populations

&#9744; Structured Decision Making

### Ecology and Evolution

&#9744; Evolution Textbook

&#9744; Ricklefs and Relyea

&#9744; Gotelli's Primer of Ecology

&#9745; Vandermeer and Goldberg

&#9744; Mittelbach

&#9744; Metapopulation ecology (TBD)

&#9744; Insect and Bird Interactions

&#9744; Ecological Niches: Linking Classical and Contemporary Approaches

&#9744; An Illustrated Guide to Theoretical Ecology

&#9744; The Unified Neutral Theory of Biodiversity and Biogeography

&#9744; Jim Brown's Macroecology

&#9744; History of the Ecosystem Concept

&#9744; Turner and Gardner

&#9744; Scales and levels

### Area sensitivity

&#9744; Key papers

&#9744; Foundational papers

### Modeling, statistics, and math

&#9744; Kery and Royle

&#9744; Hierarchical Modeling and Inference in Ecology

&#9744; The Ecological Detective

&#9744; McCarthy

&#9744; Hobbs and Hooten

&#9744; Burnham and Anderson

&#9744; Searle

&#9744; Hamilton

&#9744; Harville

&#9744; Chung

&#9744; Bayes Theorem

&#9744; Shipley

&#9744; Grace

&#9744; Quinn and Keough

&#9744; Gotelli and Ellison

&#9744; Handbook of Meta-analysis in Ecology and Evolution

# Historical perspectives {.tabset .tabset-fade .tabset-pills}
## >> 
This section contains notes on classic works in ecology and evolutionary biology or historical perspectives on the development of theories and models in ecology. 

## Foundations of Ecology 

## On the Origin of Species

## Modeling Nature

## Classic papers

## Notable ecologists {.tabset .tabset-fade .tabset-pills}

#### Robert MacArthur
#### EO Wilson
#### Charles Darwin
#### Dan Janzen
#### David Tilman
#### Alfred Wallace
#### Ernst Mayr
#### N Tinbergen
#### Stephen J Gould
#### Holling
#### Simberloff
#### Sewall Wright
#### RA Fischer
#### Richard Dawkins
#### Peter and Rosemary Grant
#### Thomas Hunt Morgan
#### JBS Haldane
#### CH Waddington
#### Niles Eldridge
#### RC Lewontin
#### G. Evelyn Hutchinson

# Current topics in ecology {.tabset .tabset-fade .tabset-pills}

## >>

In this section, I have notes on recent papers relevant to my field or interests from selected journals. Articles are chosen for inclusion in this section based on skimming Table of Contents (TOC) emails and it will be updated continuously. 

In order to keep notes in a usable format, they are stored in R objects that are then written to a master .bib file which is linked to kbibtex. Notes are also reproduced here in more readable paragraph form. 

## Trends in Ecology and Evolution

### [@Corlett2015]

The anthropocene refers to the geologic period during which human impacts have as great or greater of an impact on the planet as natural factors. One of the chief issues with the term, which was proposed by Crutzen and Stoermer in 2000, is that "-cene" implies it is an epoch that ends the Holocene rather than a division within the Holocene. 

There is debate about when the Anthropocene started (or will start) if it is an accepted period. Because not all characteristics of the Anthropocene are synchronous (e.g. extinction, ocean acidification, agriculture, greenhouse gas emisssions, etc...) it is difficult to pinpoint a start. To define a geologic period, the International Commission on Stratigraphy needs a physical marker to signify the start of the Anthropocene. 

Ecologists tend to place the start of the Anthropocene much more recently than geologists, typically somewhere in the late 1900s. Although the term really has no bearing on ecology, it shapes how we approach ecology, processes, and conservation. Within conservation biology, it has been somewhat polarizing in terms of creating both a lack of self-efficacy and longing for the past, and also by placing emphasis on the role of humans in shaping natural systems. In some ways, this is similar to the debate between preservationists and conservationists. 

### [@Fox2013]

Fox argues that Connell's Intermediate Disturbance Hypothesis should be abandoned due to a lack of empirical and theoretical support. He cites reviews that show less than 20% of studies on diversity and disturbance show the predicted result of IDH. 

The three theoretical bases for IDH are: 1) disturbance reduced densities and reduces competition, 2) disturbance prevents equilibrium, and 3) rapid fluctuations in dominant species enable coexistence. 

Fox argues that although disturbance reduces densities, it also reduces how competitive a species needs to be to exclude competitors. Coexistence is generally due to other factors besides densities. Disturbance does not disrupt a stable state, but rather increase mortality rates and increases competition between species with different growth rates. The third basis is Hutchinson's paradox of the plankton and notion of a nonequilibrium coexistence mechanism (changing conditions change relative fitness). 

Theoretical models are usually false because of assumptions, but they are still usefull. Fox argues that IDH is logically false and so its predictions do not come from the theory. 

### [@Sheil2013]

The IDH is not actually based on the three things Fox cites, and is instead based on the competition-colonization tradeoff that Fox discusses as being a better explanation for IDH-type patterns. The original IDH was developed for sessile organisms and was focused on succession and was not intended for mobile organisms. Sheil and Burslem also cite a more recent review paper that found support for the IDH in 46%, not 20%, of papers on disturbance-density relationships. 




## Ecology Letters

## Ecology

## Methods in Ecology and Evolution

### [@Harrison2011]

Meta-analysis allows us to determine the effect of one variable on another across studies, which can uncover trends that aren't apparent from many small studies with varying sample sizes and reported effects. 

Effect sizes are standardized measures of change in one variable based on change in another variable that account for sample sizes so they can be used across studies. Studies with low variance in their findings are given greater weight in determining the mean effect size. Variation in effects between studies can also be explained by other variables. 

Recommended reading: Stewart (2010) and Hillebrand (2008). 

Historically, vote-counting of significant vs non-significant relationships across studies has been used to determine support for a hypothesis. This is not a valid way to determine support because different studies have different sample sizes, which can change significance so true effects are masked. 

The effect size pulled from studies for meta-analysis depends on the type of study. Comparison studies take the mean difference between groups, regression-type studies take the correlation coefficient, and for binary responses the odds ratio is used. In addition to finding a mean effect size, meta-analysis also looks at the heterogeneity of effect size across studies. 

Recommended reading: Borenstein et al. 2009, Cooper et al. 2009, Lipsey and Wilson 2001, Koricheva et al (ND). Hedges and Olkin 1980, Gurevitch et al. 1992, Gurevitch and Hedges 1993. 

When doing a meta-analysis, first do a systematic review, then create a database of study ID, effect size, sample size, and other variables that may alter the effect size. 

Often in meta-analysis the effect size for a study needs to be calculated by the given means, variances, etc... because data is not reported in a way that is easily translated into an effect size. 

Because some variance in effect size is due to fixed effects (other variables we know influence the effect, like age or sex) but some variance between studies is random. This can be addressed by doing a mixed-effects model to test for significance of confounding variables. 

Meta-analysis is subject to publication bias because studies that find significant effects in the expected direction are more likely to be published. Funnel plots and failsafe sample size can be used to try to correct for this. Failsafe sample size is the number of unpublished studies that would need to exist to make our calculated mean effect size insignificant due to sampling error. 

## Global Change Biology

### [@Fordham2017]

Instead of taking a mechanistic approach to SDMs, this paper couples populations and demographic factors with environmental variables to model historical range shifts. 

The goal of the study was to run models at varying degrees of complexity, from correlative models to dispersal models to spatially explicit population models to see if the degree of complexity of the model impacted its predictive ability. 

The simple models assume unlimited dispersal, more complex models include species-specific dispersal limitations, and the most complex models include metapopulation dynamics of extinction and colonization. Dispersal models were similar to MigClim models, but instead of predicting a probability of a dispersal event, they predicted the probability of the number of dispersers. In the niche-population model, they included a carrying capacity for each grid cell that was a function of possible maximum abundance, the percent of land cover, suitable climate, and the minimum abundance (to represent an Allee effect). A second niche-population model did not include the percent of land cover. 

In the basic ENM models, including land use improved prediction, but when land use was included in dispersal or metapopulation models it reduced predictive ability. When more complex models did not have land use included, they were the best at predicting ranges. One reason why including land use in the more complex models did a bad job of predicting actual range dynamics is because land use was a static variable and was unknown for grid cells at the start of the model so projections may have had repercussions for later simulations when land use was known. 

## Conservation Biology

## Conservation Letters

### [@Kearney2010]

One of the primary problems with correlative models is that they implicitly include ecological processes, but only link species distributions to environmental variables. Mechanistic SDMs, in contrast, explicitly include ecological processes by linking physiological processes to limiting environmental factors. 

This study uses both a mechanistic, functional trait based model and correlative models to project the range of an Australian possum under climate change. 

The mechanistic model included properties of the fur, mass, shape, and core body temperature of possums in connection to metabolic processes as well as nesting, food, digestion, and efficiency of converting food to milk. 

The mechanistic model overpredicted the species range, though the authors say this is to be expected, whereas the correlative model underpredicted the suitable range. A combined approach does a better job than either individual approach because it incorporates changing environmental conditions and physiological processes. 




## Ecology and Evolution

## Condor

## Auk

## Nature

## Science

## Code for bibliography

```{r, eval=FALSE}

write2bib <- function(article.id="", author="", title="", journal="", volume="", number="", pages="", year="", localfile="", note=""){
    entry <- paste("@article\\{", article.id, ",",
               "author =", "\"", author, "\",",
               "journal = ", "\"", journal, "\"", ",", 
               "localfile = ", "\"", localfile, "\"", ",",
               "note = ", "\"", note, "\"", ",", 
               "number = ", "\"", number, "\"", ",", 
               "pages = ", "\"", pages, "\"", ",", 
               "title = ", "\"", "\\{", title, "\\}", "\"", ",", 
               "volume = ", "\"", volume, "\"", ",", 
               "year = ", "\"", year, "\"", ",", 
               "\\}", sep="")
    edited.entry <- gsub("\\", entry, replacement="", fixed=TRUE)
    read.bib <- readLines("~/master-bib.bib")
    updated.bib <- append(read.bib, entry)
    writeLines(updated.bib, "~/master-bib.bib")
}

#### Corlett2015 ####
id <- "Corlett2015"
auth <- "Richard T Corlett"
title <- "The Anthropocene concept in ecology and conservation"
jour <- "Trends in Ecology and Evolution"
volume <- "30"
num <-"1"
pgs <- "36--41"
yr <- "2015"
file <- "/home/aleph/Documents/readings/TREE/Corlett2015.pdf"
notes <- c("

The anthropocene refers to the geologic period during which human impacts have as great or greater of an impact on the planet as natural factors. One of the chief issues with the term, which was proposed by Crutzen and Stoermer in 2000, is that -cene implies it is an epoch that ends the Holocene rather than a division within the Holocene. 

There is debate about when the Anthropocene started (or will start) if it is an accepted period. Because not all characteristics of the Anthropocene are synchronous (e.g. extinction, ocean acidification, agriculture, greenhouse gas emisssions, etc...) it is difficult to pinpoint a start. To define a geologic period, the International Commission on Stratigraphy needs a physical marker to signify the start of the Anthropocene. 

Ecologists tend to place the start of the Anthropocene much more recently than geologists, typically somewhere in the late 1900s. Although the term really has no bearing on ecology, it shapes how we approach ecology, processes, and conservation. Within conservation biology, it has been somewhat polarizing in terms of creating both a lack of self-efficacy and longing for the past, and also by placing emphasis on the role of humans in shaping natural systems. In some ways, this is similar to the debate between preservationists and conservationists. ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### Fox2013 ####

id <- "Fox2013"
auth <- "Jeremy W Fox"
title <- "The intermediate disturbance hypothesis should be abandoned"
jour <- "Trends in Ecology and Evolution"
volume <- "28"
num <- "2"
pgs <- "86--92"
yr <- "2013"
file <- "/home/aleph/Documents/readings/TREE/Fox2013.pdf"
notes <- c("
Fox argues that Connell's Intermediate Disturbance Hypothesis should be abandoned due to a lack of empirical and theoretical support. He cites reviews that show less than 20% of studies on diversity and disturbance show the predicted result of IDH. 

The three theoretical bases for IDH are: 1) disturbance reduced densities and reduces competition, 2) disturbance prevents equilibrium, and 3) rapid fluctuations in dominant species enable coexistence. 

Fox argues that although disturbance reduces densities, it also reduces how competitive a species needs to be to exclude competitors. Coexistence is generally due to other factors besides densities. Disturbance does not disrupt a stable state, but rather increase mortality rates and increases competition between species with different growth rates. The third basis is Hutchinson's paradox of the plankton and notion of a nonequilibrium coexistence mechanism (changing conditions change relative fitness). 

Theoretical models are usually false because of assumptions, but they are still usefull. Fox argues that IDH is logically false and so its predictions do not come from the theory. 
           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)


#### Sheil2013 ####

id <- "Sheil2013"
auth <- "Douglas Sheil and David FRP Burslem"
title <- "Defining and defending Connell's intermediate disturbance hypothesis: a response to Fox"
jour <- "Trends in Ecology and Evolution"
volume <- "28"
num <- "1"
pgs <- "571--572"
yr <- "2013"
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("
The IDH is not actually based on the three things Fox cites, and is instead based on the competition-colonization tradeoff that Fox discusses as being a better explanation for IDH-type patterns. The original IDH was developed for sessile organisms and was focused on succession and was not intended for mobile organisms. Sheil and Burslem also cite a more recent review paper that found support for the IDH in 46%, not 20%, of papers on disturbance-density relationships. 
           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### Kearney2010 ####

id <- "Kearney2010"
auth <- "Michael R Kearney and Brendan A Wintle and Warren P Porter"
title <- "Correlative and mechanistic models of species distribution provide congruent forecasts under climate change"
jour <- "Conservation Letters"
volume <- "3"
num <- ""
pgs <- "203--213"
yr <- "2010"
file <- "/home/aleph/Documents/readings/cons-letters/Kearney2010.pdf"
notes <- c("

One of the primary problems with correlative models is that they implicitly include ecological processes, but only link species distributions to environmental variables. Mechanistic SDMs, in contrast, explicitly include ecological processes by linking physiological processes to limiting environmental factors. 

This study uses both a mechanistic, functional trait based model and correlative models to project the range of an Australian possum under climate change. 

The mechanistic model included properties of the fur, mass, shape, and core body temperature of possums in connection to metabolic processes as well as nesting, food, digestion, and efficiency of converting food to milk. 

The mechanistic model overpredicted the species range, though the authors say this is to be expected, whereas the correlative model underpredicted the suitable range. A combined approach does a better job than either individual approach because it incorporates changing environmental conditions and physiological processes. 
           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### Fordham2017 ####

id <- "Fordham2017"
auth <- "Damien A Fordham and Cleo Bertelsmeier and Barry W Brook and Regan Early and Dora Neto and Stuart C Brown and Sebastien Ollier and Miguel B Araujo"
title <- "How complex should models be? Comparing correlative and mechanistic range dynamics models"
jour <- "Global Change Biology"
volume <- ""
num <- ""
pgs <- "1--14"
yr <- "2017"
file <- "/home/aleph/Documents/readings/glob-change-bio/Fordham2017.pdf"
notes <- c("

Instead of taking a mechanistic approach to SDMs, this paper couples populations and demographic factors with environmental variables to model historical range shifts. 

The goal of the study was to run models at varying degrees of complexity, from correlative models to dispersal models to spatially explicit population models to see if the degree of complexity of the model impacted its predictive ability. 

The simple models assume unlimited dispersal, more complex models include species-specific dispersal limitations, and the most complex models include metapopulation dynamics of extinction and colonization. Dispersal models were similar to MigClim models, but instead of predicting a probability of a dispersal event, they predicted the probability of the number of dispersers. In the niche-population model, they included a carrying capacity for each grid cell that was a function of possible maximum abundance, the percent of land cover, suitable climate, and the minimum abundance (to represent an Allee effect). A second niche-population model did not include the percent of land cover. 

In the basic ENM models, including land use improved prediction, but when land use was included in dispersal or metapopulation models it reduced predictive ability. When more complex models did not have land use included, they were the best at predicting ranges. One reason why including land use in the more complex models did a bad job of predicting actual range dynamics is because land use was a static variable and was unknown for grid cells at the start of the model so projections may have had repercussions for later simulations when land use was known. 

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### Harrison2011 ####

id <- "Harrison2011"
auth <- "Freya Harrison"
title <- "Getting started with meta-analysis"
jour <- "Methods in Ecology and Evolution"
volume <- "2"
num <- ""
pgs <- "1--10"
yr <- "2011"
file <- "/home/aleph/Documents/readings/methods/Harrison2011.pdf"
notes <- c("
Meta-analysis allows us to determine the effect of one variable on another across studies, which can uncover trends that aren't apparent from many small studies with varying sample sizes and reported effects. 

Effect sizes are standardized measures of change in one variable based on change in another variable that account for sample sizes so they can be used across studies. Studies with low variance in their findings are given greater weight in determining the mean effect size. Variation in effects between studies can also be explained by other variables. 

Recommended reading: Stewart (2010) and Hillebrand (2008). 

Historically, vote-counting of significant vs non-significant relationships across studies has been used to determine support for a hypothesis. This is not a valid way to determine support because different studies have different sample sizes, which can change significance so true effects are masked. 

The effect size pulled from studies for meta-analysis depends on the type of study. Comparison studies take the mean difference between groups, regression-type studies take the correlation coefficient, and for binary responses the odds ratio is used. In addition to finding a mean effect size, meta-analysis also looks at the heterogeneity of effect size across studies. 

Recommended reading: Borenstein et al. 2009, Cooper et al. 2009, Lipsey and Wilson 2001, Koricheva et al (ND). Hedges and Olkin 1980, Gurevitch et al. 1992, Gurevitch and Hedges 1993. 

When doing a meta-analysis, first do a systematic review, then create a database of study ID, effect size, sample size, and other variables that may alter the effect size. 

Often in meta-analysis the effect size for a study needs to be calculated by the given means, variances, etc... because data is not reported in a way that is easily translated into an effect size. 

Because some variance in effect size is due to fixed effects (other variables we know influence the effect, like age or sex) but some variance between studies is random. This can be addressed by doing a mixed-effects model to test for significance of confounding variables. 

Meta-analysis is subject to publication bias because studies that find significant effects in the expected direction are more likely to be published. Funnel plots and failsafe sample size can be used to try to correct for this. Failsafe sample size is the number of unpublished studies that would need to exist to make our calculated mean effect size insignificant due to sampling error. 
           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### NEXT PAPER ####

id <- ""
auth <- ""
title <- ""
jour <- ""
volume <- ""
num <- ""
pgs <- ""
yr <- ""
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### NEXT PAPER ####

id <- ""
auth <- ""
title <- ""
jour <- ""
volume <- ""
num <- ""
pgs <- ""
yr <- ""
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### NEXT PAPER ####

id <- ""
auth <- ""
title <- ""
jour <- ""
volume <- ""
num <- ""
pgs <- ""
yr <- ""
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### NEXT PAPER ####

id <- ""
auth <- ""
title <- ""
jour <- ""
volume <- ""
num <- ""
pgs <- ""
yr <- ""
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)

#### NEXT PAPER ####

id <- ""
auth <- ""
title <- ""
jour <- ""
volume <- ""
num <- ""
pgs <- ""
yr <- ""
file <- "/home/aleph/Documents/readings/TREE/"
notes <- c("

           ")

write2bib(id, auth, title, jour, volume, num, pgs, yr, file, notes)


```


# Ornithology {.tabset .tabset-fade .tabset-pills}
## >>

Because I am studying birds, this section is dedicated to the field of ornithology and more specifically, my study system and trophic interactions. 

## Gill's Ornithology
## Birds and Habitat
## Ovenbirds
## Notable ornithologists {.tabset .tabset-fade .tabset-pills}
### David Lack
### Margaret Morse-Nice
### John James Audubon
### Louis Agassiz Fuertes
### Alexander Wilson
### John Gould
### Thomas Nuttall
### John Kirk Townsend
### Roger Tory Peterson
### David Sibley

# Conservation biology {.tabset .tabset-fade .tabset-pills}

## >>

Although I don't model conservation biology explicitly, contributing to knowledge about species' conservation is an implicit goal in all of my research. 

## Carroll and Fox

## Primack

## Restoring North American Birds

## Saving the World's Deciduous Forests

## Analysis and Management of Animal Populations

## Structured decision making 

Jim Nichols, Bryan Ken Williams, Andy J Royle
Sustainable resource management

# Basic evolution {.tabset .tabset-fade .tabset-pills}

## >>

I have not taken a graduate course in evolution yet and will not take it until after my general exams, so this section will be necessarily somewhat brief and will only cover basic topics in evolution. 

# General ecology {.tabset .tabset-fade .tabset-pills}

## >>

Although there are more specific sections below for population and community ecology and landscape ecology, this section is dedicated to general ecology and is based on introductory ecology texts. 

## Ricklefs and Relyea

### Chapter 1: Ecology, Evolution, and the Scientific Method

This chapter deals primarily with basic conepts in ecology, such as populations and ecosystems, so the notes are quite brief in an attempt to use time effectively. 

Ernst Haeckel gave the first formal definition of ecology and species interactions, describing what Darwin called the "economy of nature."

Ecological systems must obey the law of the conservation of mass and the conservation of energy (first law of thermodynamics). If a system has gains and losses balancing each other, it is a dynamic steady state. 

### Chapter 2: Adapatations to Aquatic Environments

$Q_{10}$ is a ratio of physiological activity at some temperature to activity at ten degrees cooler; it measures sensitivity of physiological processes to temperature.

### Chapter 3: Adaptations to Terrestrial Environments

Water potential is measured in megapascals; at 0 MPa, water is saturated; at around -1.5 MPa is the wilting point. Water potential is based on osmotic pressure. 

### Chapter 4: Adaptations to Variable Environments

Phenotypic plasticity in behavior, morphology; foraging behavior and central place foraging theory. 

### Chapter 5: Climates and Soils

Young soils have low cation exchange capacity because they have less clay and less organic matter so they don't have calcium, magnesium, potassium, or sodium in great abundance. In acidic soils, podsolization are leached from the B to E horion. Lateritic soils are not acidic, but have ben extremely weathered so clay has leached down to the E layer leaving mostly oxides. Lateritic soils are common in the tropics, whereas podsolized soils are common in New England and the Great Lakes region. 

### Chapter 6: Terrestrial and Aquatic Biomes

Biomes are mostly defined by precipitation and climate. 

Rivers can have allochtonous (similar to exogenous) inputs from outside materals, or internally produced, autochthonous (similar to endogenous) inputs of organic matter. 


### Chapter 7: Evolution and Adaptation

Polygenic traits are caused by affects of multiple genes. Pleiotropy is when one gene affects several traits. Epistasis is when one gene's expression is regulated by a different gene. 

### Chapter 8: Life Histories

Life history traits are all tied to fitness. Because of life history tradeoffs, we can generally group organisms along a continuum from slow to fast reproductive strategies. 

### Chapter 9: Reproductive Strategies

Local mate competition favors female offspring (fig wasp). 

### Chapter 10: Social Behaviors

Eusociality and haplodiploidy, types of social interactions. 


































## Gotelli

# Population and community ecology {.tabset .tabset-fade .tabset-pills}

## >>

## Vandermeer and Goldberg

### 1. Elementary Population Dynamics
$\frac{Kk-N}{K}$ from the logistic equation $\frac{dN}{dt}=rN\frac{(K-N)}{K}$ can be thought of as the available niche space that has not already been used by members of the population. 

Shinozaki and Kira came up with a simple hyperbolic equation to describe yield-density relationships that result in constant final yield. Yield is a function of density, maximum yield without density effects, and a constant. $Y=\frac{D(w_{max})}{(1+\alpha D)}$.If $k$ is the biomass of a single plant growing alone, then the biomass of a plot of plants is $w=k-\sum\alpha_{i,j}w_j$ which can be rearranged to get the average biomass of an individual in the plot as $w' = \frac{k}{(1+\alpha ' D)}$. Yoda reasoned that the $-\frac{3}{2}$ constant of yield decrease with increasing density was due to trees being equated with cubes and with self-thinning this is the expected constant.

### 2. Life History Analysis
Semelparous strategy is to do either reproduction or maintenance in a given year, contrasted with iteroparity in which a bit of energy is put into both. Depending on if it is a good year in terms of resources or a bad year, one or the other strategy can be favored. For an annual plant, fecundity has to be higher than fecundity for a perennial in order for it to be the strategy. 

### 3. Projection Matrices: Structured Models
Structured populations at their stable age distribution grow exponentially, but density dependence can be incorporated into Leslie (population projection) matrices. Lewis actually came up with the matrices in 1942, three years before Leslie. In a Leslie matrix, the first row is the fecundities (generally, $m$) and the diagonal is the survival probabilities at that age class. 

The rate of increase of a stable age population is the dominant eigenvalue of the projection matrix. If we have a sufficiently large $t$, we can say that $N_{t+1}=\lambda N_t$ where $\lambda$ is the diagonal of the matrix resulting from multiplying $P$ by itself $n$ times. We can rewrite this as $PN_t = \lambda N_t$, which when rearranged and factored is $(P-\lambda I)N_t =0$ with $I$ as the identity matrix. The determinant of this is $\lambda^2 - gm$ and the roots of this are the eigenvalues; the larger is the dominant eigenvalue and represents the rate of increase in the population. 

When a population follows $PN_t=\lambda N_t$, then we can say that there is a column vector $u$ that satisfies the equation $Pu = \lambda u$ and a vector $v$ that satisfies $P'v = \lambda v$. $u$ is the right eigenvector and represents the contribution of other stages to that stage (stable stage distribution) and $v$ is the left eigenvector that represents the contributions of a stage to all other stages (reproductive value).

The <b>sensitivity</b> of a matrix is how the rate of population growth changes with changes to $p_{ij}$. Because changes are in the units of the matrix and mortality and natality vary widely (e.g. grasses with thousands of seeds), ecologists use elasticity which is based on logs. Elasticity is $e_{ij}=\frac{\partial(\ln\lambda)}{\partial(\ln p_{ij}}$. This sums to 1, so elasticity of each individual element of the matrix can be seen as proportional sensitivity. 

### 4. A Closer Look at the 'Dynamics' in Population Dynamics
Attractor means a singularity in stable equilibrium, a repellor is a singularity that leads to unstable equilibrium. These can be visualized as a magnet falling through a beaker of water and the trajectory as viewed from the bottom matches with the phase plane diagrams; swirling the bucket creates oscillators which leads to damped oscillations if it is an oscillatory point attractor. There are also periodic attractors/repellers, like a bundt cake pan. Strange attractors are attractors that are flat from a distance, but up close are bumpy, so once a system reaches them, then it bounces around on that bumpy space. With strange attractors, we want to know the range over which a system can move and also its morphology.

The eigenvalue is the rate at which a system tends towards a point attractor and can be thought of as the slope of the line connecting the tips of the arrows pointing towards an attractor. If the eigenvalue has a non-zero complex part (i.e. it has $i$) then it is oscillatory; if the complex part is positive, it is a repellor and if it is negative then it is an attractor. <br><br>

In a one-dimensional graph created by stair-stepping, the slope of the function as it crosses the 45-degree plane is the eigenvalue. <br><b><center>oscillatory repellor $< 1 <$ oscillatory attractor $< 0 <$ point attractor  $< 1 <$ point repellor</center></b>

Systems are structurally unstable if a small change in $r$ leads to a change in the behavior of the system. These are bifurcation points and can lead to period doubling; Hopf bifurcations are a special case for continuous systems with differential equations. Bifurcations create attractors (nodes) and repellors (saddles) and are sometimes called "blue sky bifurcations" because they come out of the blue. Basin coundary collisions are a special type of bifurcation in which a point and strange attractors basins collide. Enough bifurcations lead to chaos via the period doubling route to chaos. 

## Metapopulation Ecology
## Mittelbach
## Insect and Bird Interactions
## Ecological Niches: Linking Classical and Contemporary Approaches

# Macroecology and theoretical ecology {.tabset .tabset-fade .tabset-pills}
## >>
## An Illustrated Guide to Theoretical Ecology
## The Unified Neutral Theory of Biodiversity and Biogeography

## Jim Brown's Macroecology

## History of the Ecosystem Concept

# Landscape ecology {.tabset .tabset-fade .tabset-pills}

## >>

## Turner and Gardner

## Scales and levels

Cash

# Area sensitivity {.tabset .tabset-fade .tabset-pills}

## >>

## Key papers

## Foundational papers

# Ecological modeling {.tabset .tabset-fade .tabset-pills}

## >>

## Kery and Royle

## Hierarchical Modeling and Inference in Ecology

## The Ecological Detective

# Species Distribution Models {.tabset .tabset-fade .tabset-pills}

## >>

# Bayesian statistics {.tabset .tabset-fade .tabset-pills}

## >>

## McCarthy

## Hobbs and Hooten

# Information theoretic approach {.tabset .tabset-fade .tabset-pills}

## >>



# Linear algebra {.tabset .tabset-fade .tabset-pills}

## >>

## Searle

## Hamilton

## Harville

# Probability theory {.tabset .tabset-fade .tabset-pills}

## >>

## Chung

## Bayes Theorem

# Experimental design {.tabset .tabset-fade .tabset-pills}

## >>

# Meta-analysis {.tabset .tabset-fade .tabset-pills}
## >>
## Handbook of Meta-analysis in Ecology and Evolution {.tabset .tabset-fade .tabset-pills}

[@Koricheva2013]

### Section I: Introduction and Planning
#### Place of meta-analysis among other methods of research synthesis

Historically, synthesis of information in ecology has been done either as a narrative review or by "vote counting" significant findings. What is surprising about the way in which synthesis has historically been done is that it is much less rigorous than standard procedures for analyzing data in a study. Research synthesis should be held to just as high, if not higher, a standard as research. 

One of the primary criticisms of narrative reviews is that they are rarely done systematically and the process of including or excluding articles is opaque. This can lead to apparently contradicting support for hypotheses and limits the usefulness of narrative reviews. Some researchers combine probabilities or Z-scores for research synthesis, but this is not a good approach because one low P-value will almost always ensure the null is rejected. Combining probabilities also doesn't take the direction of the relationship into account. 

Meta-analysis combines effect sizes from multiple studies to determine support for a hypothesis. It expresses effect sizes on a common scale to be able to combine studies. Effect sizes include both direction and magnitude. Meta-analysis can even detect an effect across studies even if none of the studies found a significant effect. 

> "An important contribution of meta-analyses can be to identify gaps in the literature where more research is needed."

Meta-analysis is based on using systematic review to identify articles from which to draw effect sizes. Systematic reviews can be done without meta-analysis, but meta-analysis cannot be done without systematic reviews without producing erroneous results. 

Necessary information to conduct a meta-analysis, such as variances, is often not reported in ecology papers. Because of this problem, some syntheses combine meta-analysis and vote-counting.

#### The procedure of meta-analysis in a nutshell

> "You can think of an effect size as a P-value corrected for sample size with the direction of the relationship also provided"

This quote doesn't seem to represent my understanding of effect sizes, which show the direction and magnitude of an effect corrected for variance?

Doing a meta-analysis requires a formal protocol that specifies the search strategy and inclusion criteria. This criteria should include relevance to your question and also study quality. 

Determining where articles will be pulled from is an important step. For theoretical or non-applied topics, journals are fine, but in conservation many results are in the gray literature associated with government reports or unpublished studies. Similarly, the language of sources included can bias results. 

Although software developed by Lajeunesse allows for controlling for phylogenetic nonindependence of effect sizes for studies of the same species, it is not entirely necessary to do this for every meta-analysis in ecology. 

### Section II: Initiating a meta-analysis

#### First steps in beginning a meta-analysis

Designing a meta-analysis involves the same basic principles of designing a scientific experiment. The population of interest needs to be identified as well as the relationship and response of interest. Comparators should also be specified if response to a treatment is the research question. 

Including large numbers of sub-groups in your population or potential moderating variables can increase the complexity of data collection and analysis but can make things easier down the road because you won't need to go back and do a second round of data extraction. Systematic review reduces bias and meta-analysis reduces imprecision; without doing a systematic review a meta-analysis is biased. 

The protocol should not only specify resources and search strategies, but also the manner in which data will be extracted from articles. Post-hoc revisions can be done, but only if there is good justification for them. For example, if the findings contradict widely held beliefs, exploring moderator effects may be necessary. 

#### Gathering data: searching literature and searching criteria

A scoping search should be done before a full review to determine if enough data exists for a meta-analysis. 

When searching for data, it is not enough to just rely on electronic databases. Reference lists of included articles should be scanned because articles from pre-1980 may not be electronically archived. The gray literature should also be searched, especially if the research question is applied in some way. In particular, conference proceedings, dissertation research, and chapters in books offer the most useful materials in gray literature. In addition, government websites often have reports with useful data. 

Although ideally all the gray literature and unpublished data would be searched and included in a meta-analysis, this can be time-consuming as it is not properly indexed which can bias results. 

Keyword selection is critical because poor selection of keywords can bias search results. If databases have Boolean capabilities, this is a good way to comibne search strings. Use wildcards to capture spelling differences and include both British and American spellings. 

For inclusion criteria, specify the subject, the treatment or study aspect of interest, the response types, comparators or controls, and the type of study. 

The number of papers excluded at the title, abstract, and full-text stage should be recorded. A second reviewer should use the same criteria on a subsample of the total sample to get a kappa score of agreement. 

#### Extraction and critical appraisal of data

For a one-person meta-analysis, a spreadsheet is fine for tracking data. Each article should get an accession number, though if multiple effect sizes come from a paper there needs to be an additional identifier for this. There should also be a separate, but linked, bibliographic database. It is helpful to have a column specifying where data was extracted from for each effect size so that it is easy to go back and check. 

There should also be a library of all the papers not included in the sample and a justification for not including them. For meta-analysis, this includes rejecting articles that don't contain enough data to calculate an effect size. 

If data is presented graphically, it is possible to use software like DataThief, Graphclick, or ImageJ to get data values from a figure. 

Data extraction protocols need to specify what to do for time series data. There also needs to be clear protocols for converting degrees of freedom to sample size, standard deviation to standard error, and converting test statistics to effect sizes. 

Most meta-analysis software requires a single spreadsheet of data, rather than a relational database, so information stored in separate places (e.g. bibliographic information) will need to be merged prior to analysis. 

Some meta-analyses score studies on quality and reject poorly designed or conducted studies to avoid biasing results, however this is a very subjective step in the process. Alternatively, any potential quality issues can be coded for during data extraction and studies can be subsequently removed if the potential quality issues do indeed influence results. 

#### Effect sizes: conventional choices and calculations

Effect size can be measured in different ways, but they all attempt to represent magnitude and direction of a relationship in a common metric that can be interpreted and compared across studies. In meta-analysis, we give greater weight to effects that have higher precision. The same effect size measurement must be used for all studies included in a meta-analysis. 

The original effect size measure was Glass's $\Delta$, though Hedges' $g$ and Cohen's $d$ are also used and are variations on $\Delta$. 

> "The most common (and preferred) metric in use today is known as Hedges' $d$". 

Hedges' $d$ includes a correction for small sample size. The $d$ scores can also be converted to $Z$-scores to determine the percentage of scores different between two treatments. 

Instead of using the standardized mean difference between comparators as a measure of effect size, the ratio of means, or response ratior $R$ can be used. Because ratios aren't great for statistical analysis because of their asymmetrical distributions, they are generally log-transformed. Usually, $R$ is used if both comparator responses have the same direction. If values are log-transformed, they should be back-transformed for interpretation. If the signs of responses are opposite, then $d$ should be used. 

There is no effect size for contingency tables greater than $2 \times 2$. For these types of tables, the rate of response, $P$ is the number showing a response divided by the total number. From this rate $P$, the rate difference, rate ratio, or odds ratio can be used as the effect size. $RD$ causes apparent heterogeneity between studies so it is not widely used. The odds ratio $OR$ is the most commonly used effect size for contingency tables. It is equal to probability of occurence divided by probability of non-occurrence of an event or response. 

For correlation data, Pearson's correlation coefficient $r$ can be used but needs to be transformed with Fisher's $z$-transformation because as $r$ approaches $1$ it gets skewed. It is also possible to transform other statistics into $r$. 

A Mantel correlation represents distance or dissimilarity between two square $n \times n$ matrices excluding the diagonal. Because the matrices have pairwise comparisons, the distribution for Mantel's correlation is not the same as for other correlation coefficients. These should NOT be combined in a meta-analysis with $r$ because they have very different meanings. 

For simple linear regressions, the slope of the regression can be used as the effect size if all studies have the same units of measurement. They also must include a measure of the variance of the estimate of the slope; if slopes are rescaled to have the same units then the variances must also be rescaled. If the variance of the slope is not provided, it is possible to estimate it from other information (see Box 6.7 for details). 

#### Using other metrics of effect size in meta-analysis

For observational studies, it is not possible to get an effect size as a mean difference between comparators because there is no comparator. For example, in studies of biodiversity indices, the index value itself is the value of interest. 

Other measures of effect size need to be drawn from data that is normally distributed. It is safe to assume that means based on large sample sizes, regression parameter estimates, rate estimates, and proportions (if based on large sample sizes) are likely normally distributed. 

See Table 7.1 on page 74 for examples of effect estimates for common effects of interest such as rate of occurrence or probability of success. 

Count data is assumed to follow a Poisson distribution in which the mean and variance are equal, so only the count (which can be estimated from the rate) is necessary for comparison across studies. Count data needs to be on the same scale (i.e. in the same units) and if it needs to be rescaled then the variance must also be rescaled. 

Proportional, or binary data, has a distribution that depends on the probability of success and the number of trials ($p$ and $n$). Generally, $\hat{p}$ needs to transformed either as $\log(\hat{p})$ or as $\sin^{-1}\sqrt{\hat{p}}$ or as $\log({\hat{p}/(1- \hat{p})})$. 

Per capita and whole population parameters have different statistical properties which means their variances are estimated differently. The population parameter is typically estimated based on a sample size $n$, so the variance depends on the distribution, $n$, and for whole population parameters, $N$. 

If studies report a new parameter with unknown distribution, it is typically best to derive a more common effect size, such as $r$ from the data. 

If effect sizes are non-normal, a GLM can be used instead of standard meta-analysis models. 

Because some effect sizes are corrected in original studies, it is necessary to perform a sensitivity analysis to check for influences of assumptions about estimates and distribution of data. 








### Section III: Essential analytic models and methods

#### Statistical models and approaches to inference


If the interest is to quantify sources of variation among studies based on confounding factors, then those factors need to be inluded in the meta-analysis model, typically as a meta-regression model. Sources of variation are included as either a fixed-effects or random-effects model. 

Bayesian meta-analysis models can be done if there is sufficient prior information. 

If we assume that there is no variation across studies and that sampling variability is only within studies, then we can do a a simple fixed-effects model. Generally in ecology we assume that there are differences across studies, in which case we can use a random-effects model. A random-effects model assumes variance within studies and also that the effects from studies are grouped around a mean effect for the pouplation. 

Beyond a random-effects model that allows for variation, we can also build models that characterize the sources of variation. These can be mixed-effects models. A meta-regression includes factors and covariates to characterize variation among studies. In a mixed model, we can treat some characteristics of studies as fixed effects and then allow for variation, or random effects, within subgroups. These are often called mxed models but are actually random-effects meta-regression. There can also be hierarchical/nested models and factorial models. 

Models can either be linear or distributional; linear are easier to interpret, but distributional models work for non-normal distributions and can be more complex. 

<ul><li>$T_i$ is the effect of interest

</li><li>$\sigma_i^2$ is the within-study sampling variance of $T_i$

</li><li>$S_i^2$ is the estimate of $\sigma_i^2$; it is not the same as the sample variance; for example if $T_i$ is a sample mean, then $S_i^2$ is the standard error of the mean.

</li><li>$\theta_i$ is the true effect that is estimated by $T_i$

</li><li>$\mu$ is the overall effect across studies</li>

<li>$\tau^2$ is the between study variance 

</li><li>$e_i$ is the residual offseting $T_i$ from $\theta_i$</li></ul>

A <b>fixed effects</b> model is written as:
<center>$T_i = \theta_i + e_i; e_i \sim N(0, \sigma_i^2)$

$\theta_i=\mu$

</center>or as 

<center>$T_i \sim N(\theta_i, \sigma_i^2)$

$\theta_i=\mu$. </center>

A <b>random effects</b> model is written as:
<center>$T_i = \theta_i + e_i; e_i \sim N(0, \sigma_i^2)$

$\theta_i=\mu + \epsilon_i; \epsilon_i \sim N(0, \tau^2)$</center>

or as 

<center>$T_i \sim N(\theta_i, \sigma_i^2)$

$\theta_i \sim N(\mu, \tau^2)$. </center>


For a <b>meta-reression model</b>, we also have $p$ characteristics $X_1, X_2, X_3, \ldots, X_p$ that may explain variability across studies. Then we have:

<center>$T_i = \theta_i + e_i; e_i \sim N(0, \sigma_i^2)$

$\theta_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \ldots + \beta_pX_{pi} + \epsilon_i; \epsilon_i \sim N(0, \tau^2)$</center>

This meta-regression model can then be a simple linear regression, factorial model, mixed-effects, or hierarchical model. Hierarchical models assume subgroups have a mean and that these means are distributed around the overall mean. 

> "This model says, 'each study's estimate, $T_i$, is distributed normally with a true mean $\theta_i$; these $\theta_i$ are normally distributed around subgroup means, $\Psi_j$,and these subgroup means are normally distributed around an overall mean \mu.'"

Most statistical inference in meta-analysis in ecology is based on the method of moments or least-squares which estimate global means and look at subgroups. These are both popular methods because they resemble the least squares approach in regression but can only be used in simpler meta-analysis; hierarchical data requires likelihood or Bayesian approaches. 

Using MLE allows all parameters to be estimated at once which gives a better overall sense of the model. 

Weighting effect sizes from different studies by variance and sample size is a core aspect of meta-analysis. The precision of an effect size, given as the inverse of the sampling variance, is used to weight studies. 

When choosing between fixed effects and random effects (if there is no theoretical reason to choose random effects), you can do a test of homogeneity to see if your studies are from the same population or if you should do a random-effects model. However, these tests have low power and will often suggest using fixed effects even when there is heterogeneity across studies. The only reason to actually do this test is if you were planning to use fixed effects and want to make sure you aren't forgetting to consider some factor that would have caused heterogeneity but there needs to be a good a priori reason to do so. 

#### Moment and least-squares based approaches to meta-analytic inference

Moment estimators of weights in fixed-effects and random-effects models can be calculated as follows.

<b>Fixed effects:</b>
<center> $w_k = \frac{1}{v_k}$</center>
<b>Random effects:</b>
<center>$w_k = \frac{1}{v_k + \hat{\sigma}^2}$</center>


The mean effect size across all studies is then the weighted mean of effect sizes from each study with variance $s_{\mu}^2$. 

<center>$\hat{\mu} = {\sum \limits_{k=1}^K w_k \hat{\theta}_k \over \sum\limits_{k=1}^K w_k}$</center>

<center>$s_{\mu}^2 = {1 \over \sum\limits_{k=1}^K w+k}$</center>

Once a mean effect size has been calculated, confidence intervals should be constructed to check that it does not overlap the null (typically an effect size of 0) or a t-test should be done. Typically, a confidence interval around the mean will be larger for a random-effects than for fixed-effects model. 

Heterogeneity is measured with $Q_T$, which is the weighted sums of squares. This test only make sense if doing a fixed-effects model to see if studies assumed to be homogenous were actually heterogenous. An alternative to $Q_T$ is $I^2$, which compares $Q_T$ to the expected value if homogeneity is assumed. Confidence intervals can be constructed around $I^2$. 

> "[$I^2$] can be interpreted as the percentage of total heterogeneity that can be attributed to between-study variance."

Resampling techniques can be used with small sample sizes that don't adhere to distribution assumptions. Bootstrapping can be used to generate confidence intervals for mean effect size. 

#### Maximum likelihood approaches to meta-analysis
















# Structural equation modeling {.tabset .tabset-fade .tabset-pills}

## >>

## Shipley

## Grace

# Basic statistics {.tabset .tabset-fade .tabset-pills}

## >>

## Quinn and Keough {.tabset .tabset-fade .tabset-pills}

### 1. Introduction 

#### 1.1   Scientific method

Bacon advocated the inductive approach, said that a law or theory is true if there is lots of evidence for it and nothing contradicts it. Popper advocated the hypothetico-deductive approach of disproving theories because proof is logically impossible.

Verbal models describe an idea with words. Empiric models are mathematical descriptions of relationships, like fitting observations to a curve and are usually statistical models. Theoretic models are also mathematical models, but they describe a process and generalizations drawn from them are better than from empiric models.

Imre Lakatos suggested that science needs core theories that are almost universally accepted and not tested. These core theories are then supplemented by auxiliary theories that are tested frequently and replaced if they are not supported

#### 1.2 Experiments and other tests
Inferences from manipulative experiments are stronger than from natural experiments because of the inability to control for other factors as well.

#### 1.3 Data, observations, and variables
A random variable $X$ is a variable whose values aren't known until we sample it; each value of $X$ is $x$.  

#### 1.4 Probability
Process uncertainty is an actual change, like taking samples at two different times and having the mean really change between then. Sample uncertainty is due to random sampling. There is also measurement uncertainty.

The probability that $A$ will happen is equal to $1$ minus the probability that it does not happen and vv. $P(A) = 1- P(\tilde{} A)$. In joint probability distributions, you can also have the probability that both $A$ and $B$ happen as $P(A \cup B) = P(A)+P(B) - P(A \cap B)$ which means that we take the probability of the union (total) as equal to the probability of each $A$ and $B$ minus their intersection (overlap).

In conditional probability, want to know probability of $A$ given $B$ which is $P(A|B) = \frac{P(A \cap B)}{P(B)}$. Using Bayes theorem, we can assess posterior probability of observing the data given prior likelihood. $P(\theta | X) = \frac{P(X | \theta)P(\theta)}{P(X)}$ where $X$ is our data. In this case, $P(\theta)$ is unconditional, $P(X | \theta)$ is the likelihood of observing the $X$ if $\theta$ is true and $P(\theta | X)$ is the posterior.

#### 1.5 Probability distributions
The expected value of a random variable is the mean of its probability distribution. In ecology, variables tend to follow a common set of probability distributions (these are different than distributions for statistics). The <b>exponential distribution</b> is used for the time to first occurrence, like in failure analysis; its only parameter is $\lambda$ and $\frac{1}{\lambda}$ is the mean time to first occurrence. The <b>Weibull distribution</b> can be either positive skewed or symmetric depending on its parameters; it usually only has two but can have three and is used to model failure rates. The <b>beta distribution</b> has two parameters and can be shaped either like a U or J or be symmetrical; it is used as the prior probability distribution for dichotomous variables in Bayesian analysis. Bernoulli trials only have success or failures, like coin flips, so they are described by a <b>binomial distribution</b>. The <b>Poisson distribution</b> is usually for things with rare and discrete occurrences, like point counts, and is all integers; the mean of the Poisson distribution equals its variance. The <b>negative binomial distribution</b> has two parameters, mean and dispersion, and is used to measure degree of clumpiness.

Theoretical distributions are used for hypothesis testing and describe the distribution of statistics rather than variables. In ecology, the four most frequently used are the $z$-distribution which is the normal distribution; <b>Student's t</b> which starts resembling the normal with increasing sample size; $\chi^2$ which is the square of the normal and is bounded by $0$ and $\infty$; and the <b>F distribution</b> which is the ratio of two $\chi^2$ each divided by their own df. 

### 2. Estimation


#### 2.1 Samples and populations
In frequentist framework, population parameters are fixed but unknown; they are not random variables and have no distribution. In Bayesian, they have distributions. 

#### 2.2 Common parameters and statistics
<b>L-estimators</b> are based on ordering data from smallest to largest. The median, trimmed mean (remove 5 percent at tail ends first) and the Winsorized mean (replacing the trimmed values with the nearest remaining value) are all L-estimators. <b>M-estimators</b> include a measure of variability in estimation. Huber M-estimator and Hampel M-estimator use different approaches of weighting observations. M-estimators require iteration and are not common except in robust regression.<b>R-estimators</b> are based on ranks and are the basis for most non-parametric tests. Hodges-Lehmann estimator is the only common one.

Sum of squares (SS) is the sum of the squared standard deviations from the sample mean. 

#### 2.3 Standard errors and confidence intervals for the mean
The z transformation conversts a normal distribution to the standard normal distribution with a mean of 0 and variance of 1. Sample means are normallly distributed random variables; as the sample size of sample means increases they tend more towards a normal distribution which is the basis for the Central Limit Theorem. The mean of the sample means is the population mean. The <b>standard error of the mean</b> is the standard deviation of the sample means. It is "error" because using $\hat{y}$ as an estimator of $\mu$ has some error.

We can transform any sample mean into one from a standard normal distribution with $z=\frac{\hat{y}-\mu}{\sigma_{\hat{y}}}$ which then gives us confidence intervals of how close our sample mean $\hat{y}$ is to the true population mean $\mu$. In practice, we don't know the standard error for our sample, so we use the sample standard deviation to estimate it which is why we end up with a $t$ distribution. The $t$ distribution approaches the normal as sample size increases because it is imprecise at small sample sizes. The $t$ distribution's shape changes with degrees of freedom.

We can only use the above methods for tests that assume normality. For sample statistics with non-normal or unknown distributions, we generally need to use resampling to find the standard error.

Because variances are positive, confidence intervals for them are based on the $\chi^2$ distribution. 
#### 2.4 Explanation of degrees of freedom
Degrees of freedom is the number of observations that are "free to vary" when estimating variance; we can calculate the mean, so that is fixed, which is why it is $n-1$ instead of $n$ for df. Wider confidence intervals: Increasing variance, reducing sample size, and increasing the level of confidence all result in wider confidence intervals.

#### 2.5 Methods for estimating parameters
The maximum likelihood estimator (ML) is based on trying to find the parameter value assuming the data is fixed. It is a function of $f(y_i; \theta)$ which is the probability distribution of values of $Y$ for possible values of the parameter $\theta$. The likelihood function is $L(y; \theta) = \prod\limits_{i=1}^{n}f(y_i; \theta)$ but because no one likes working with products, instead we use the log-likelihood which is $L(\theta)=\ln\bigg[\prod\limits_{i=1}^{n}f(y_i; \theta)\bigg] = \sum\limits_{i=1}^{n}\ln[f(y_i;\theta)]$

Ordinatory least squares (OLS) estimators always have exact solutions. They are commonly used for linear models. OLS attempts to find the value that minimizes the sum of squared differences between $y_i$ and $\mu$. OLS is not suited for models with non-normal response variables, so GLMs and nonlinear models use ML. ML is more time-intensive, but generally the same as OLS or better except when estimating the population variance $\sigma^2$ at small sample sies. 

#### 2.6 Resampling methods for estimation
Efron developed bootstrapping - resampling from the same sample with replacement. Then the bootstrap estimate uses all these multiple resampled samples to come up with means and standard deviations from the bootstrapped samples.

Jackknifing involves creating pseudovalues by iteratively removing an onvservation and then taking the mean and standard deviation of all the pseudovalues of the mean. It requires less computing power than bootstrapping but assumes independence of samples (not true) and normality. 

#### 2.7 Bayesian inference - estimation
Bayesian inference incorporates prior knowledge of a parameter value as degrees-of-belief. Population parameters are random variables in Bayesian methods. Because we don't always have prior knowledge, we can use uninformative priors in Bayesian methods. Doing this lets the data determine the posterior distribution.

The Bayesian equivalent of a confidence interval is a credible interval,whihch suggests directly that there is a specific probability (e.g. 95 percent) that the value lies within our credible range. The posterior is mostly influenced by the data unless the prior has a small variance.



### 3. Hypothesis testing
#### 3.1 Statistical hypothesis testing
Fisher's approach to hypothesis testing is to set up $H_0$, choose test statistic (e.g. $t$), collect data and compare sample data to test statistic distribution, then determine the $P$ value (probability of obtaining sample value or one more extreme if $H_0$ is true) and then reject or retain $H_0$ depending on $P$. Neyman and Pearson (1933) took this a step further and said that we should set the significance level in advance and stick to it. In the Neyman-Pearson scheme, there is also an alternative hypothesis $H_A$ that is accepted as true if $H_0$ is rejected. Neyman and Pearson also came up with Type I ($\alpha$, falsely rejecting $H)0$) and Type II ($\beta$, failing to reject $H_0$) error.

An F-test is the ratio of two sample variances to see if they are the same or different according to the F-distribution. A $t$ test relies on three assumptions: samples are from a normally distributed population, equal variance between populations, and observations are randomly sampled. 

#### 3.2 Decision errors
Type I errors can only occur when $H_0$ is true whereas Type II errors can only occur when $H_0$ is false. In environmental decision making Type II errors are more serious because of failing to address a problem, whereas a Type I error just means that you've done mitigation for no reason. 


#### 3.3. Other testing methods
When variances or sample sizes are unequal, can do a robust parametric test that calculates $t$ differently (e.g. Behrens-Fisher, Welch test, etc...). If data is not normally distributed, then the Wilcox $H$ test (based on M-estimators) is more appropriate. <b>Randomization tests</b> take the comparison between two groups, then reassign groups iteratively and calculate the proportion of the reassigned differences to the actual difference to see how many reiterations are significantly different. This approach is useful for time series and for data with unknown distributions or no opportunity to randomly sample (e.g. collections specimens).

The Mann-Whitney test (also sometimes Mann-Whitney-Wilcoxon because it was developed twice) is a rank-based non-parametric test. You rank all your observations, regardless of their group, then calculate the sum of the ranks for each sample (we would expect them to be similar if no difference); repeat this for random group reassignment and then do normal tests on it. For paired samples, can do the <b>Wilcoxon signed-rank test</b> in which each part of a pair gets a $+$ or $-$ and then you look at the sum of positive and negative ranks. The only good reason to do rank-based tests instead of parametric ones is if the distribution is weird, transformations can't normalize it, or there are outliers. 


#### 3.4 Multiple testing
Bonferroni can be used to adjust significance levels to control for Type I errors in families of tests, but the sequential Bonferroni (Holm or Hochberg) is preferred because it gives more power to individual tests. 

#### 3.5 Combining results from statistical tests
P-values can be combined for tests of the same $H_0$, which produces a total measure of the $H_0$ and we can reject or not reject an overall effect. This is still only a reject/not-reject test, and a meta-analysis should combine effect sizes. 

#### 3.6 Critique of statistical hypothesis testing

### 4. Graphical exploration of data
#### 4.1 Analysis with graphs
Multimodal (two or more peaks) data can't reliably be tested with parametric or non-parametric tests and should be split into multiple populations. Similarly, if you have a lot of zeros, the best thing to do is to break tests up into response/non-response and then look at effects within the groups that do respond while ignoring the zeros. 

### 5. Correlation and regression
#### 5.1 Correlation analysis
Bivariate normal distribution is defined by the mean and standard deviation of each variable and the correlation coefficient.

Covariance depends on the units of measurement; standardizing it (dividing by standard deviations of the two variables) produces the Pearson correlation, which is the strength of the relationship. $r$ can also be found by finding the covariance of two standardized variables with a mean of $0$ and variance of $1$.

Covariance is symbolized $\sigma_{Y_1Y_2}$ and is estimated as

\large $s_{Y_1Y_2}=\frac{\sum\limits_{i=1}^n (y_{i1}-\bar{y_1})(y_{i2}-\bar{y_2})}{n-1}$. 

\small
Correlation is symbolized $\rho_{Y_1Y_2}$ and is estimated as

$r_{Y_1Y_2}=\frac{\sum\limits_{i=1}^n(y_{i1}-\bar{y_1})(y_{i2}-\bar{y_2})}{\sqrt{\sum\limits_{i=1}^n(y_{i1}-\bar{y_1})^2\sum\limits_{i=1}^n(y_{i2}-\bar{y_2})^2}}$
\small
If $\rho \neq 0$, then the distribution of $r$ is complex and the standard error should be found with resampling or bootstrapping. Confidence intervals for $r$ can be found using z-transformation. If $H_0$ is that $\rho = 0$, then we can use $t = \frac{r}{s_r}$ and comapre this to a t-distribution. If $H_0$ is that $\rho =$ any value but $0$, then you have to use a test based on z-transformation. 

<b>Assumptions</b> of Pearson correlation coefficient are that the joint probability distrbution is bivariate normal, which means each of the individual variables must also have normal distributions and there is a linear relationship between them. Scatterplots will help you find non-linear patterns, which can sometimes be corrected with transformation.

Spearman's correlation coefficient ($r_s$) is the same as Pearson's but the variables are rank-transformed first. Kendall's tau $\tau$ does a similar thing. These measures are good for detecting non-linear monotonic relationships, but not non-monotonic relationships. 


#### 5.2 Linear models
A linear model is basically just response = model + error where the model is some number of predictors and their parameter values. <b>General</b> linear models means that either categorical or continuous predictor variables are allowed. <b>Generalized</b> linear models allow non-normal distributions and relationships between the mean and variance of the response. 

#### 5.3 Linear regression analysis
A $\beta$ coefficient is a standardized regression coefficient to make comparisons between models easier since the value of a regression coefficient depends on the units used. It is calculated by multiplying the regression coefficient by the ratio of the standard deviation of $X$ to the standard deviation of $Y$. $\beta = b_1\frac{s_X}{s_Y}$.

Confidence intervals are biconcave bands around a regression line; 95% of the time the true regression line will lie within them.

With linear models, we can partition the variability in the response into that which is due to its relationsihp with predictor variables and also variability due to random error. The part due to the relationship is the sum of squares of the regression and is $\sum\limits_{i=1}^n(\hat{y_i}-\bar{y})^2$ where the df is the number of parameters minus $1$. The sum of squares due to random error is the difference between the predicted value of $\hat{y_i}$ and the actual value of $y_i$, the residual. This is $\sum\limits_{i=1}^n(y_i-\hat{y_i})^2$. The regression and residual SS (sum of squares) are additive to get the total SS, including adding the df. Because it is additive, this will always get bigger, so MS (mean square) is used instead. MS is the SS divided by degrees of freedom and is not additive. Both MS for regression and residual estimate $\sigma_{\epsilon}^2$ except the regression MS also has an extra source of variation based on the strength of the relationship between the predictor and response.

$r^2$ is not the absolute fit of a model to data; it is how much better a model that includes one or more parameters fits the data than a model with no slope parameter.

<b>Diagnostics</b> of regression models are used to check that a model does indeed fit the data and there aren't extreme outliers with extra influence on it. Leverage is how much each $x_i$ influences the model, so high leverage influences the model more. Leverage is usually symbolized $h_i$ because of the hat matrix ($H$). Leverage is included as part of Cook's D. Residuals can be studentized by standardizing them and incorporating leverage in order to compare them and detect outliers. Cook's D measures the influence of each observation on the slope of the regression line by incorporating size of residuals and leverages.

Model II regression is when $X$ values are not fixed and are instead take random values. We can use OLS to make predictions still, but if we want to describe the relationship, OLS might not be the best approach. MA is major-axis regression and minimizes perpendicular distance to the regression line, instead of verticle like with OLS. There is also reduced major-axis regression (RMA) which minimizes the sums of the areas of right triangles from the observations to the regression line.

Robust regression techniques minimize the amount of influence that outliers have on regression and aren't as sensitive to if distributions don't meet assumptions. Least absolute deviations (LAD) minimizes absolute values of residuals instead of their squares, so extreme values of residuals don't influence the model as much since they aren't squared. Huber M-estimators can be used in which you switch from OLS to LAD as you get farther from zero. 




#### 5.4 Relationship between regression and correlation
Regression is related to correlation by the ratio of the standard deviations of $Y$ and $X$ so with an OLS estimate, $b_Y=r_{YX} \frac{s_Y}{s_X}$.

#### 5.5 Smoothing
Smoothers, like running means, Loess, splines, and kernels are used to figure out a relationship without specifying any parameters or type of model, so they can show linear or non-monotonic relationships. They rely on moving windows to smooth data, but the smoothing value is chosen by the user. Standard errors for smoothers are based on bootstrapping and hypothesis tests are based on randomization. 

### 6. Multiple and complex regression
#### 6.1 Multiple linear regression analysis
Partial regression coefficients measure the change in the response with a one unit change in that predictor if all other predictors are held constant. When looking at the analysis of variance, we are interested now in the variability explained by each of the predictor variables in addition to splitting it up into regression and residuals. To test partial regression slopes as being different from $H_0$, we compare the fit of the full model to reduced models. When checking SS, we need to divide by df to get MS; for reduced models this is the number of predictors in the full minus the number of predictors in the reduced model (so when checking one partial regression slope, df is 1). For any of the predictor variables, we can also test that it is different from zero with a $t$ statistic. If you use $F$ tests instead, you can check subsets of regression slopes at the same time.

To compare relative importance of predictors, we can compare the fit of reduced models or can standardize the partial regression slopes (i.e. make them into $/beta$-coefficients). Some people also use partial standard deviations to standardize. To deal with collinearity, standardized (or even just centered) predictor variables should be used.

<b>Assumptions</b> of multiple regression is that error terms are normally distributed, that each $X$ is fixed (i.e. not random, could resample with the same $X$ values), that the predictor variables are not correlated with each other, and that you have more observations than predictors because otherwise the matrix fails.

Bivariate scatterplots can be used to check for multicollinearity of predictor variables. To view the relationship between $Y$ and $X_j$, we need to see what the relationship is when all other $X$s are held constant, so we look at the partial regression plot (also called added variable plot).

The problem with collinearity is that if your predictors are correlated, when you invert the matrix, you divide by the determinant. Having rows (or columns) that are correlated makes your determinant close to zero, so when you divide by the determinant it is sensitive to small changes so the inverted matrix and all the partial regression slopes are unstable. We can check for collinearity with scattterplots and should also check tolerance values. Tolerance is $1-r^2$ from the regression of $X_i$ against all the other predictor variables. Low tolerance means that $X_i$ is correlated with at least one of the other $X$s; anything less than $0.1$ is bad.

One problem with just removing highly correlated predictors is that then their parameter estimates are biased since they are also accounting for the removed variables. Another way to deal with collinearity is to do principal components regression by extracting $p$ principal components from the matrix and regressing $Y$ on them instead, since the principal components are uncorrelated. However, this is difficult to interpret, so we can back-calculate partial regression slopes for the original variables based on the principal components. 


### 7. Design and power analysis
#### 7.1 Sampling
If a population is stratified and you want to use stratified sampling to make sure you capture all the variation and don't miss any by random chance with simple random sampling, you need to include the relative proportion of the population that each stratum is when calculating the mean or sample proportionally. For cluster sampling (think of it hierarchically like tree > branch > leaves), do simple random sampling at each cluster stage. Cluster sampling is best suited for hierarchical or nested models. The primary problem with systematic sampling is that unknown gradients can bias the samples. 

To determine the size of the sample needed to estimate the true population mean, you use the $z$ value from a standard normal distribution for confidence levels, the variance of the population based on a previous data, and the allowed difference between the sample mean and population mean. $n \geq \frac{z^2 \sigma^2}{d^2}$
#### 7.2 Experimental design
Pseudoreplication, confounding factors, randomization; considerations for field and manipulative experiments. 

#### 7.3 Power analysis
Power is the long-term probability of detecting an effect if it occurs and is $1 - \beta$ if $\beta$ symbolizes the probability of making a Type II error. Power is some function of the effect size ($ES$), sample size ($n$), variance ($\sigma^2$), and desired significance level ($\alpha$). Power $\propto \frac{ES \alpha \sqrt{n}}{\sigma}$. 

The Minimum Detectable Effect Size (MDES), also called a reverse power analysis, determines what effect size you can detect given variation in your available sample. This is most often used for controls to show that even a small effect would have been detected in your control smaples. 

One of the main problems with post hoc power calculation ("observed power") is that it uses the variance and effect size from your sample. Post hoc power analysis corresponds to $\rho$-values, so it tells you nothing new. 

There are no firm rules for meaningful effect sizes since it depends on what is biologically significant in a system. One useful way to determine a sampling regime is to plot potential effect sizes against sample size and find the most reasonable size that is logistically feasible and yields a desired effect size. 

#### 7.4 General issues and hints for analysis
To do a power analysis, you need to have an estimate of variation and the statistical model you are going to apply to your data. 

### 8. Comparing groups or treatments - analysis of variance
The goal of ANOVA is to partition the variance in a response variable so that it is explained by categorical factors.
#### 8.1 Single factor (one way) designs

#### 8.2 Factor effects
#### 8.3 Assumptions
#### 8.4 ANOVA diagnostics
#### 8.5 Robust ANOVA
#### 8.6 Specific comparison of means
#### 8.7 Tests for trends
#### 8.8 Testing equality of group variances
#### 8.9 Power of single factor ANOVA
#### 8.10 General issues and hints for analysis




### 9. Multifactor analysis of variance
#### 9.1 Nested (hierarchical) designs
#### 9.2 Factorial designs
#### 9.3 Pooling in multifactor designs
#### 9.4 Relationship between factorial and nested designs
#### 9.5 General issues and hints for analysis

### 10. Randomized blocks and simple repeated measures: unreplicated two factor designs
#### 10.1 Unreplicated two factor experimental designs
#### 10.2 Analyzing RCB and RM designs
#### 10.3 Interactions in RCB and RM models
#### 10.4 Assumptions
#### 10.5 Robust RCB and RM analyses
#### 10.6 Specific comparisons
#### 10.7 Efficiency of blocking
#### 10.8 Time as a blocking factor
#### 10.9 Analysis of unbalanced RCB designs
#### 10.10 Power of RCB or simple RM designs
#### 10.11 More complex block designs
#### 10.12 Generalized randomized block designs
#### 10.13 RCB and RM designs and statistical software
#### 10.14 General issues and hints for analysis


## Gotelli and Ellison {.tabset .tabset-fade .tabset-pills}


# References

#

